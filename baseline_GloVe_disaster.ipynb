{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13deef7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  Classify disaster tweets\n",
    "# Source : https://www.kaggle.com/competitions/nlp-getting-started/data\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "plt.style.use(style='seaborn')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8773717",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the data\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test  = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "426d1f27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          0  \\\n",
       "id                                                        1   \n",
       "keyword                                                 NaN   \n",
       "location                                                NaN   \n",
       "text      Our Deeds are the Reason of this #earthquake M...   \n",
       "target                                                    1   \n",
       "\n",
       "                                               1  \\\n",
       "id                                             4   \n",
       "keyword                                      NaN   \n",
       "location                                     NaN   \n",
       "text      Forest fire near La Ronge Sask. Canada   \n",
       "target                                         1   \n",
       "\n",
       "                                                          2  \\\n",
       "id                                                        5   \n",
       "keyword                                                 NaN   \n",
       "location                                                NaN   \n",
       "text      All residents asked to 'shelter in place' are ...   \n",
       "target                                                    1   \n",
       "\n",
       "                                                          3  \\\n",
       "id                                                        6   \n",
       "keyword                                                 NaN   \n",
       "location                                                NaN   \n",
       "text      13,000 people receive #wildfires evacuation or...   \n",
       "target                                                    1   \n",
       "\n",
       "                                                          4  \n",
       "id                                                        7  \n",
       "keyword                                                 NaN  \n",
       "location                                                NaN  \n",
       "text      Just got sent this photo from Ruby #Alaska as ...  \n",
       "target                                                    1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68503457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>location</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           0  \\\n",
       "id                                         0   \n",
       "keyword                                  NaN   \n",
       "location                                 NaN   \n",
       "text      Just happened a terrible car crash   \n",
       "\n",
       "                                                          1  \\\n",
       "id                                                        2   \n",
       "keyword                                                 NaN   \n",
       "location                                                NaN   \n",
       "text      Heard about #earthquake is different cities, s...   \n",
       "\n",
       "                                                          2  \\\n",
       "id                                                        3   \n",
       "keyword                                                 NaN   \n",
       "location                                                NaN   \n",
       "text      there is a forest fire at spot pond, geese are...   \n",
       "\n",
       "                                                 3  \\\n",
       "id                                               9   \n",
       "keyword                                        NaN   \n",
       "location                                       NaN   \n",
       "text      Apocalypse lighting. #Spokane #wildfires   \n",
       "\n",
       "                                                      4  \n",
       "id                                                   11  \n",
       "keyword                                             NaN  \n",
       "location                                            NaN  \n",
       "text      Typhoon Soudelor kills 28 in China and Taiwan  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e08e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove URLs and HTML\n",
    "import re\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r\"<.*?>\")\n",
    "    return html.sub(r\"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f47b9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\n",
    "    \"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "    u\"\\U00002702-\\U000027B0\"  \n",
    "    u\"\\U000024C2-\\U0001F251\"\n",
    "    \"]+\",\n",
    "    flags=re.UNICODE,\n",
    "    )\n",
    "    return emoji_pattern.sub(r\"\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96357e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove puntuation\n",
    "import string\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72c195fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train.text.map(lambda x: remove_URL(x))\n",
    "train[\"text\"] = train.text.map(lambda x: remove_html(x))\n",
    "train[\"text\"] = train.text.map(lambda x: remove_emoji(x))\n",
    "train[\"text\"] = train.text.map(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45dfaf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a235755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20f862c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"text\"].map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e5769c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            deeds reason earthquake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       residents asked shelter place notified officer...\n",
       "3       13000 people receive wildfires evacuation orde...\n",
       "4       got sent photo ruby alaska smoke wildfires pou...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding bridge collapse nearb...\n",
       "7609    ariaahrary thetawniest control wild fires cali...\n",
       "7610                      m194 0104 utc5km volcano hawaii\n",
       "7611    police investigating ebike collided car little...\n",
       "7612    latest homes razed northern california wildfir...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text # cleaning is done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "41402d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\eraym\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20f72d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Embeddings\n",
    "# Using GloVe\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "def create_corpus_tk(df):\n",
    "    corpus = []\n",
    "    for text in train[\"text\"]:\n",
    "        words = [word.lower() for word in word_tokenize(text)]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f06fdf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = create_corpus_tk(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5d49dae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7613\n"
     ]
    }
   ],
   "source": [
    "num_words = len(corpus)\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5add1db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['deeds', 'reason', 'earthquake', 'may', 'allah', 'forgive', 'us']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "311a03ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "train_size = int(train.shape[0] * 0.8)\n",
    "\n",
    "train_sentences = train.text[:train_size]\n",
    "train_labels = train.target[:train_size]\n",
    "\n",
    "test_sentences = train.text[:train_size]\n",
    "test_labels = train.target[:train_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfe87ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install keras\n",
    "# !pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b2389aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "max_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f4c0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e8626d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da34e981",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_padded = pad_sequences(\n",
    "    train_sequences, maxlen=max_len, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ec5fed6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3739,  696,  235, ...,    0,    0,    0],\n",
       "       [  71,    3,  129, ...,    0,    0,    0],\n",
       "       [1448, 1186, 1882, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 151,    1, 1256, ...,    0,    0,    0],\n",
       "       [1256,  448,   15, ...,    0,    0,    0],\n",
       "       [ 151,  204,  539, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "122d0cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
    "test_padded   = pad_sequences(\n",
    "    test_sequences, maxlen=max_len, truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca09fdf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3739,  696,  235, ...,    0,    0,    0],\n",
       "       [  71,    3,  129, ...,    0,    0,    0],\n",
       "       [1448, 1186, 1882, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 151,    1, 1256, ...,    0,    0,    0],\n",
       "       [1256,  448,   15, ...,    0,    0,    0],\n",
       "       [ 151,  204,  539, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "103e724e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deeds reason earthquake may allah forgive us\n",
      "[3739, 696, 235, 41, 1282, 3740, 14]\n"
     ]
    }
   ],
   "source": [
    "print(train.text[0])\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a5c70f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 15470\n"
     ]
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"Number of unique words:\", len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d76edbac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1554"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index[\"turkish\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "634eb2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3739, 696, 235, 41, 1282, 3740, 14]\n"
     ]
    }
   ],
   "source": [
    "print(test_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "759570e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nowplaying sinking fast never north east unsigned radio listen\n"
     ]
    }
   ],
   "source": [
    "print(train.text[train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6d5987f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the embedding dictionary\n",
    "# Source: https://nlp.stanford.edu/projects/glove/\n",
    "embedding_dict = {}\n",
    "with open(\"glove.twitter.27B/glove.twitter.27B.100d.txt\",\"r\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values  = line.split()\n",
    "        word    = values[0]\n",
    "        vectors = np.asarray(values[1:], \"float32\")\n",
    "        embedding_dict[word] = vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "97db31e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((num_words, 100))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i<num_words:\n",
    "        emb_vec = embedding_dict.get(word)\n",
    "        if emb_vec is not None:\n",
    "            embedding_matrix[i] = emb_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "00ffe960",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Baseline Model with GloVe\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from keras.initializers import Constant\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(\n",
    "    Embedding(\n",
    "        num_words,\n",
    "        100,\n",
    "        embeddings_initializer=Constant(embedding_matrix),\n",
    "        input_length=max_len,\n",
    "        trainable=False,\n",
    "    )\n",
    ")\n",
    "model.add(LSTM(100, dropout=0.1))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = Adam(learning_rate=3e-4)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "33f6976a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "191/191 [==============================] - 13s 49ms/step - loss: 0.5315 - accuracy: 0.7345 - val_loss: 0.4549 - val_accuracy: 0.8071\n",
      "Epoch 2/20\n",
      "191/191 [==============================] - 8s 44ms/step - loss: 0.4558 - accuracy: 0.8053 - val_loss: 0.4390 - val_accuracy: 0.8176\n",
      "Epoch 3/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.4440 - accuracy: 0.8099 - val_loss: 0.4300 - val_accuracy: 0.8186\n",
      "Epoch 4/20\n",
      "191/191 [==============================] - 9s 47ms/step - loss: 0.4339 - accuracy: 0.8143 - val_loss: 0.4113 - val_accuracy: 0.8278\n",
      "Epoch 5/20\n",
      "191/191 [==============================] - 9s 46ms/step - loss: 0.4214 - accuracy: 0.8210 - val_loss: 0.4153 - val_accuracy: 0.8250\n",
      "Epoch 6/20\n",
      "191/191 [==============================] - 9s 46ms/step - loss: 0.4135 - accuracy: 0.8266 - val_loss: 0.4102 - val_accuracy: 0.8278\n",
      "Epoch 7/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.4133 - accuracy: 0.8215 - val_loss: 0.3858 - val_accuracy: 0.8430\n",
      "Epoch 8/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.3980 - accuracy: 0.8335 - val_loss: 0.3701 - val_accuracy: 0.8465\n",
      "Epoch 9/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.3947 - accuracy: 0.8322 - val_loss: 0.3631 - val_accuracy: 0.8504\n",
      "Epoch 10/20\n",
      "191/191 [==============================] - 9s 47ms/step - loss: 0.3821 - accuracy: 0.8399 - val_loss: 0.3612 - val_accuracy: 0.8501\n",
      "Epoch 11/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.3779 - accuracy: 0.8406 - val_loss: 0.3845 - val_accuracy: 0.8438\n",
      "Epoch 12/20\n",
      "191/191 [==============================] - 9s 47ms/step - loss: 0.3680 - accuracy: 0.8447 - val_loss: 0.3346 - val_accuracy: 0.8652\n",
      "Epoch 13/20\n",
      "191/191 [==============================] - 9s 46ms/step - loss: 0.3565 - accuracy: 0.8557 - val_loss: 0.3355 - val_accuracy: 0.8686\n",
      "Epoch 14/20\n",
      "191/191 [==============================] - 9s 46ms/step - loss: 0.3584 - accuracy: 0.8491 - val_loss: 0.3150 - val_accuracy: 0.8722\n",
      "Epoch 15/20\n",
      "191/191 [==============================] - 9s 47ms/step - loss: 0.3445 - accuracy: 0.8594 - val_loss: 0.3106 - val_accuracy: 0.8782\n",
      "Epoch 16/20\n",
      "191/191 [==============================] - 9s 46ms/step - loss: 0.3368 - accuracy: 0.8655 - val_loss: 0.3224 - val_accuracy: 0.8713\n",
      "Epoch 17/20\n",
      "191/191 [==============================] - 9s 46ms/step - loss: 0.3271 - accuracy: 0.8683 - val_loss: 0.3028 - val_accuracy: 0.8765\n",
      "Epoch 18/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.3175 - accuracy: 0.8718 - val_loss: 0.2771 - val_accuracy: 0.8946\n",
      "Epoch 19/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.3099 - accuracy: 0.8752 - val_loss: 0.2953 - val_accuracy: 0.8767\n",
      "Epoch 20/20\n",
      "191/191 [==============================] - 9s 45ms/step - loss: 0.3092 - accuracy: 0.8768 - val_loss: 0.2696 - val_accuracy: 0.8995\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_padded,\n",
    "    train_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(test_padded,test_labels),\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "702a7711",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(test.text)\n",
    "padded    = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3bdb0157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102/102 [==============================] - 2s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "pred     = model.predict(padded)\n",
    "pred_int = pred.round().astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5a2759cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.86226964],\n",
       "       [0.9131665 ],\n",
       "       [0.9808945 ],\n",
       "       ...,\n",
       "       [0.986975  ],\n",
       "       [0.72680193],\n",
       "       [0.6390544 ]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0fbc841f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([235,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "64d16bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.9538511]], dtype=float32)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(padded[5].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f4b072f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check inverse\n",
    "reverse_word_index = dict([(value,key) for (key,value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "befa0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(text):\n",
    "    return \" \".join([reverse_word_index.get(i, \"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3329f4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'earthquake'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(sequences[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb6e29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
